{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e53e355",
   "metadata": {},
   "source": [
    "## Natural language processing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b09b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: conda environment data_review is set up for this notebook\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415aab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy model to use\n",
    "spacy_mod = 'en_core_web_lg' # lg is the \"large\" option, other options: en_core_web_md, en_core_web_sm (med and small)\n",
    "\n",
    "# note: need to downlad the spacy_mod, in command line use \"python -m spacy download en_core_web_lg\"\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960f4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "test = pd.read_csv('./data/drugsComTest_raw.csv')\n",
    "train = pd.read_csv('./data/drugsComTrain_raw.csv')\n",
    "merge = [train,test]\n",
    "merged_data = pd.concat(merge,ignore_index=True)\n",
    "bc_merged = merged_data[merged_data['condition'] == 'Birth Control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c2605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This removes the HTML escaped charaters \n",
    "# Melissa note: for some reason the first time I run this it gives error but when I run through a second time is fine? \n",
    "from html import unescape\n",
    "\n",
    "def clean_review(text):\n",
    "    return unescape(text.strip(' \"\\'')).replace('\\ufeff1', '')\n",
    "\n",
    "bc_merged.review = bc_merged.review.apply(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def55df3",
   "metadata": {},
   "source": [
    "## SpaCy lemmatization\n",
    "\n",
    "Note: first I am not doing the stop words just to see what I am working with. It seems that some people seem to think they are actually useful to keep so not having the stopwords removed may be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e02e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language processor\n",
    "nlp = spacy.load(spacy_mod, disable = ['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "381a9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 22, no prior children, I have endometriosis & was told Mirena was the best option. The implementation process was hell. The worst pain I have ever been in. I fainted & had to have a friend pick me up. I left it in for 10 months. Within those 10 months I'd gained 18 pounds (on a 5 foot tall person that's horrible), I was moody, had the worst cramps, hair loss, acne, horrible headaches, the appetite of a sumo wrestler, & didn't want to do anything. I felt lazy and borderline depressed. They don't tell you those side effects for obvious reasons. Yesterday I finally had it removed. Being traumatized by having it put in I about made myself sick with nerves. The removal was quick and painless and I already feel like myself again.\n"
     ]
    }
   ],
   "source": [
    "# pick one example review to work with in example\n",
    "\n",
    "num = 100 # choose random number to get different review\n",
    "review_example = bc_merged.review.iloc[num]\n",
    "print(review_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf31a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be 22 , no prior child , I have endometriosis & be tell Mirena be the good option . the implementation process be hell . the bad pain I have ever be in . I faint & have to have a friend pick I up . I leave it in for 10 month . within those 10 month I have gain 18 pound ( on a 5 foot tall person that be horrible ) , I be moody , have the bad cramp , hair loss , acne , horrible headache , the appetite of a sumo wrestler , & do n't want to do anything . I feel lazy and borderline depressed . they do n't tell you those side effect for obvious reason . yesterday I finally have it remove . be traumatize by have it put in I about make myself sick with nerve . the removal be quick and painless and I already feel like myself again .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see what the lemmatization of this review is\n",
    "doc = nlp(review_example)\n",
    "review_token = \" \".join([token.lemma_ for token in doc])\n",
    "print(review_token)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5265070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(review_example.replace('/', ' / '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b2a8e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "704984ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am 22, no prior children, I have endometriosis & was told Mirena was the best option. The implementation process was hell. The worst pain I have ever been in. I fainted & had to have a friend pick me up. I left it in for 10 months. Within those 10 months I'd gained 18 pounds (on a 5 foot tall person that's horrible), I was moody, had the worst cramps, hair loss, acne, horrible headaches, the appetite of a sumo wrestler, & didn't want to do anything. I felt lazy and borderline depressed. They don't tell you those side effects for obvious reasons. Yesterday I finally had it removed. Being traumatized by having it put in I about made myself sick with nerves. The removal was quick and painless and I already feel like myself again."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4947c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens[10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e3c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4397b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review2token(text):\n",
    "    # tokenize\n",
    "    doc = nlp(text)\n",
    "    # now to turn into a list of strings that are the tokens\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        #skip spaces\n",
    "        if token.text.isspace():\n",
    "            continue \n",
    "        \n",
    "        words.append(token.text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c290cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type is a string\n",
    "a = review2token(review_example)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77f449ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ef1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae953e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "de413e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize all the reviews in the data set\n",
    "\n",
    "# this is a big data set so I am just going to run for less for now!\n",
    "#reviews_lemma = [nlp(review) for review in bc_merged.review]\n",
    "n = 1000\n",
    "bc_merged_sub = bc_merged.iloc[:n]\n",
    "\n",
    "reviews_lemma = [review2token(review) for review in bc_merged_sub.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "22ca1c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "602d3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_lemma[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c1a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23618f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88c882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638e170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ea58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d36d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f1286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "626a14bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "21235cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each token appears\n",
    "token_count = {}\n",
    "\n",
    "for rev in reviews_lemma:\n",
    "    for token in rev:\n",
    "        token_count[token] = token_count.get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7a996a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7005"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80d68938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15005a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add34ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "af8ba788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the counts\n",
    "token_count = dict(sorted(token_count.items(), key = lambda pair: pair[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "555564c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Top 500 most frequent tokens\n",
       "\n",
       "| Token | Count | Token | Count | Token | Count | Token | Count | Token | Count |\n",
       "| ---: | :---- | ---: | :---- | ---: | :---- | ---: | :---- | ---: | :---- |\n",
       "| I | 7005 | . | 5860 | and | 3129 | , | 3079 | the | 2539 |\n",
       "| it | 2281 | to | 2081 | my | 1967 | a | 1957 | have | 1558 |\n",
       "| for | 1467 | was | 1399 | had | 1169 | of | 1058 | on | 1022 |\n",
       "| this | 923 | n't | 874 | but | 870 | that | 815 | is | 789 |\n",
       "| in | 740 | ! | 682 | me | 673 | been | 671 | period | 634 |\n",
       "| not | 617 | with | 574 | so | 559 | about | 549 | months | 548 |\n",
       "| pill | 543 | 've | 541 | 'm | 526 | no | 495 | birth | 472 |\n",
       "| control | 467 | now | 441 | 's | 436 | first | 414 | get | 384 |\n",
       "| month | 379 | did | 376 | weight | 366 | am | 364 | all | 362 |\n",
       "| at | 357 | after | 355 | do | 354 | My | 350 | ) | 348 |\n",
       "| periods | 347 | ( | 344 | out | 342 | It | 339 | 3 | 339 |\n",
       "| The | 334 | just | 331 | got | 327 | or | 320 | would | 320 |\n",
       "| very | 318 | days | 316 | like | 315 | years | 311 | - | 310 |\n",
       "| as | 296 | side | 294 | has | 292 | only | 288 | started | 284 |\n",
       "| be | 284 | time | 282 | taking | 279 | because | 276 | are | 265 |\n",
       "| you | 265 | 2 | 255 | cramps | 251 | week | 250 | effects | 246 |\n",
       "| mood | 245 | bleeding | 242 | acne | 237 | before | 234 | bad | 228 |\n",
       "| sex | 226 | never | 225 | since | 223 | any | 219 | gain | 219 |\n",
       "| when | 218 | from | 215 | which | 214 | feel | 212 | year | 212 |\n",
       "| up | 211 | more | 207 | also | 202 | one | 200 | if | 200 |\n",
       "| day | 197 | back | 193 | weeks | 192 | swings | 190 | spotting | 185 |\n",
       "| then | 181 | two | 178 | some | 176 | take | 175 | than | 173 |\n",
       "| really | 173 | getting | 172 | having | 168 | pain | 168 | were | 164 |\n",
       "| every | 164 | over | 161 | will | 160 | pregnant | 157 | an | 157 |\n",
       "| they | 150 | light | 149 | even | 148 | 5 | 147 | doctor | 147 |\n",
       "| could | 145 | little | 145 | almost | 142 | This | 141 | went | 141 |\n",
       "| drive | 141 | 6 | 139 | off | 139 | pills | 138 | still | 138 |\n",
       "| few | 138 | other | 135 | gained | 134 | can | 133 | much | 133 |\n",
       "| 4 | 131 | ever | 131 | great | 130 | made | 129 | heavy | 129 |\n",
       "| recommend | 129 | cramping | 127 | experience | 126 | far | 124 | going | 123 |\n",
       "| good | 120 | No | 118 | painful | 118 | go | 117 | felt | 117 |\n",
       "| But | 116 | body | 116 | love | 116 | headaches | 115 | inserted | 115 |\n",
       "| took | 114 | put | 113 | last | 112 | pounds | 110 | noticed | 110 |\n",
       "| always | 108 | insertion | 108 | normal | 105 | its | 103 | say | 102 |\n",
       "| well | 100 | does | 99 | again | 99 | horrible | 98 | ago | 97 |\n",
       "| skin | 97 | used | 95 | So | 95 | / | 95 | thing | 94 |\n",
       "| lot | 94 | pregnancy | 94 | different | 94 | being | 93 | After | 93 |\n",
       "| implant | 93 | ca | 93 | stopped | 92 | experienced | 92 | during | 89 |\n",
       "| 10 | 88 | better | 88 | shot | 88 | ... | 86 | removed | 86 |\n",
       "| nothing | 86 | while | 85 | depression | 84 | problems | 84 | away | 84 |\n",
       "| IUD | 84 | by | 82 | think | 81 | know | 81 | switched | 80 |\n",
       "| work | 80 | until | 80 | nausea | 80 | too | 79 | there | 79 |\n",
       "| try | 79 | : | 79 | Implanon | 79 | three | 78 | long | 78 |\n",
       "| same | 77 | want | 77 | your | 77 | stop | 76 | second | 75 |\n",
       "| due | 75 | reviews | 75 | \" | 75 | worst | 74 | what | 74 |\n",
       "| anything | 74 | though | 73 | symptoms | 73 | depressed | 73 | tried | 73 |\n",
       "| use | 73 | using | 72 | Nexplanon | 72 | sure | 72 | & | 71 |\n",
       "| most | 70 | something | 70 | completely | 69 | life | 69 | another | 68 |\n",
       "| moody | 68 | around | 68 | these | 68 | old | 68 | extremely | 68 |\n",
       "| pretty | 68 | anxiety | 68 | thought | 67 | gotten | 67 | definitely | 66 |\n",
       "| taken | 65 | next | 65 | how | 65 | help | 64 | them | 64 |\n",
       "| emotional | 63 | decided | 63 | told | 63 | fine | 62 | down | 62 |\n",
       "| 1 | 62 | Mirena | 62 | worse | 60 | half | 60 | crazy | 60 |\n",
       "| times | 59 | way | 59 | lighter | 59 | bit | 58 | make | 58 |\n",
       "| terrible | 58 | change | 58 | blood | 58 | said | 57 | lost | 57 |\n",
       "| hair | 57 | i | 57 | best | 57 | lasted | 56 | where | 56 |\n",
       "| less | 56 | everything | 56 | 7 | 56 | severe | 56 | And | 55 |\n",
       "| things | 55 | 8 | 55 | happy | 55 | issues | 55 | into | 55 |\n",
       "| However | 55 | breasts | 54 | face | 54 | wanted | 54 | new | 54 |\n",
       "| absolutely | 54 | through | 54 | When | 53 | pack | 53 | right | 53 |\n",
       "| ? | 53 | once | 53 | couple | 53 | everyone | 53 | effective | 52 |\n",
       "| worth | 52 | patch | 51 | longer | 51 | boyfriend | 51 | feeling | 51 |\n",
       "| arm | 51 | Also | 50 | see | 50 | hormones | 50 | form | 49 |\n",
       "| 'll | 49 | gone | 49 | anymore | 49 | reason | 49 | regular | 49 |\n",
       "| sometimes | 48 | 15 | 48 | anyone | 48 | .. | 48 | 20 | 48 |\n",
       "| everyday | 48 | If | 48 | Loestrin | 48 | breast | 48 | give | 47 |\n",
       "| later | 46 | break | 46 | medication | 46 | BC | 46 | past | 46 |\n",
       "| trying | 45 | insurance | 45 | starting | 45 | switching | 45 | bleed | 45 |\n",
       "| Lo | 45 | Not | 44 | people | 44 | Nuvaring | 44 | finally | 44 |\n",
       "| notice | 44 | effect | 44 | hurt | 44 | loss | 44 | myself | 44 |\n",
       "| cycle | 43 | negative | 43 | 'd | 43 | constantly | 43 | super | 43 |\n",
       "| yet | 42 | problem | 42 | works | 42 | today | 41 | soon | 41 |\n",
       "| tired | 41 | read | 41 | worked | 41 | Now | 41 | point | 41 |\n",
       "| night | 41 | Skyla | 40 | who | 40 | already | 40 | straight | 40 |\n",
       "| without | 40 | she | 40 | worry | 39 | nauseous | 39 | eat | 39 |\n",
       "| actually | 39 | appetite | 39 | gave | 38 | awful | 38 | free | 37 |\n",
       "| many | 37 | changed | 37 | clear | 37 | caused | 37 | stomach | 37 |\n",
       "| Ortho | 37 | maybe | 36 | became | 36 | switch | 36 | migraines | 36 |\n",
       "| cry | 36 | Sprintec | 36 | may | 35 | low | 35 | person | 35 |\n",
       "| helped | 35 | hoping | 35 | loved | 35 | done | 34 | those | 34 |\n",
       "| however | 34 | kids | 34 | increased | 34 | makes | 34 | start | 34 |\n",
       "| enough | 34 | At | 33 | came | 33 | 2015 | 33 | baby | 33 |\n",
       "| Then | 33 | eating | 33 | constant | 33 | within | 33 | husband | 33 |\n",
       "| Since | 33 | ring | 33 | Depo | 33 | Fe | 33 | least | 33 |\n",
       "| lbs | 32 | They | 32 | found | 32 | For | 32 | bled | 32 |\n",
       "| hours | 32 | keep | 32 | sore | 32 | libido | 31 | why | 31 |\n",
       "| job | 31 | between | 31 | pains | 31 | ; | 31 | nexplanon | 31 |\n",
       "| except | 31 | able | 31 | sick | 31 | recently | 31 | 9 | 31 |\n",
       "| Tri | 31 | high | 31 | he | 31 | end | 30 | method | 30 |\n",
       "| remember | 30 | slight | 30 | women | 30 | uncomfortable | 30 | find | 30 |\n",
       "| cause | 30 | place | 30 | 18 | 30 | Overall | 30 | else | 30 |\n",
       "| extreme | 30 | hormonal | 30 | minutes | 30 | procedure | 30 | ended | 29 |\n",
       "| immediately | 29 | bc | 29 | hope | 29 | honestly | 29 | come | 29 |\n",
       "| A | 29 | skyla | 29 | nt | 29 | usually | 29 | 're | 29 |\n",
       "| recommended | 29 | changes | 29 | daily | 28 | lower | 28 | should | 28 |\n",
       "| we | 28 | deal | 28 | part | 28 | major | 28 | whole | 28 |\n",
       "| pressure | 28 | each | 28 | mild | 28 | entire | 28 | hour | 28 |\n",
       "| experiencing | 27 | Insertion | 27 | bloating | 27 | easy | 27 | brand | 27 |\n",
       "| prior | 27 | depo | 26 | 30 | 26 | lose | 26 | began | 26 |\n",
       "| working | 26 | angry | 26 | non | 26 | 3rd | 26 | size | 26 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display a table of the most frequent words\n",
    "\n",
    "# keep it within our vocabulary\n",
    "show_words = 500\n",
    "columns = 5\n",
    "\n",
    "markdown_rows = []\n",
    "markdown_rows.append(f'#### Top {show_words} most frequent tokens\\n')\n",
    "markdown_rows.append('| Token | Count '*columns + '|')\n",
    "markdown_rows.append('| ---: | :---- '*columns + '|')\n",
    "\n",
    "row = ''\n",
    "for index, (word, count) in enumerate(token_count.items()):\n",
    "    if index >= show_words: break\n",
    "    if index%columns == 0 and row:\n",
    "        markdown_rows.append(row + '|')\n",
    "        row = ''\n",
    "    row += f'| {word} | {count} '\n",
    "if row:\n",
    "    markdown_rows.append(row + '|')\n",
    "    \n",
    "IPython.display.Markdown('\\n'.join(markdown_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711232f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58e47a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(text):\n",
    "    \"\"\"text -> tokenize -> lemmatize/normalize\"\"\"\n",
    "    \n",
    "    tokens = nlp(\n",
    "        # also split on \"/\"\n",
    "        text.replace('/', ' / '),\n",
    "        \n",
    "        # we only need tokenizer and lemmas, so disable the rest\n",
    "        disable=['tagger', 'parser', 'ner']\n",
    "    )\n",
    "    \n",
    "    lexemes = []\n",
    "    for token in tokens:\n",
    "        \n",
    "        # sometimes whitespace gets recognized as a token...\n",
    "        if token.text.isspace():\n",
    "            continue\n",
    "            \n",
    "        # prefer more general representations\n",
    "        # but only if they have an embedding\n",
    "        if nlp.vocab[token.lemma_.lower()].has_vector:\n",
    "            lexeme = token.lemma_.lower()\n",
    "        elif nlp.vocab[token.norm_.lower()].has_vector:\n",
    "            lexeme = token.norm_.lower()\n",
    "        else:\n",
    "            lexeme = token.lower_\n",
    "        \n",
    "        lexemes.append(lexeme)\n",
    "        \n",
    "    return lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a4e2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100 # choose random number to get different review\n",
    "text = bc_merged.review.iloc[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4bc29e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am 22, no prior children, I have endometriosis & was told Mirena was the best option. The implementation process was hell. The worst pain I have ever been in. I fainted & had to have a friend pick me up. I left it in for 10 months. Within those 10 months I'd gained 18 pounds (on a 5 foot tall person that's horrible), I was moody, had the worst cramps, hair loss, acne, horrible headaches, the appetite of a sumo wrestler, & didn't want to do anything. I felt lazy and borderline depressed. They don't tell you those side effects for obvious reasons. Yesterday I finally had it removed. Being traumatized by having it put in I about made myself sick with nerves. The removal was quick and painless and I already feel like myself again.\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a047758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 15:09:23,661] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,662] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'am'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,662] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '22'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,662] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,663] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'no'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,663] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'prior'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,664] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'children'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,664] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,665] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,665] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'have'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,665] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'endometriosis'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,667] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '&'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,668] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,668] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'told'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,669] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Mirena'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,669] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,670] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,670] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'best'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,671] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'option'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,671] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,671] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'The'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,672] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'implementation'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,672] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'process'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,673] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,674] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'hell'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,674] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,675] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'The'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,675] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'worst'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,676] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'pain'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,676] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,677] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'have'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,677] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'ever'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,678] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'been'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,678] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 15:09:23,679] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,679] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,680] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'fainted'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,681] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '&'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,682] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'had'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,683] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,683] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'have'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,684] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'a'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,684] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'friend'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,685] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'pick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,685] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'me'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,686] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'up'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,686] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,687] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,687] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'left'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,688] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'it'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,688] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,689] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,689] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '10'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,690] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'months'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,690] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,691] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Within'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,691] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'those'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,692] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '10'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,692] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'months'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,693] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,693] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ''d'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,694] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'gained'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,695] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '18'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,696] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'pounds'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,697] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '('. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,698] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'on'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,698] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'a'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,699] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '5'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 15:09:23,700] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'foot'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,701] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'tall'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,702] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'person'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,703] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'that'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,704] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ''s'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,708] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'horrible'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,708] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ')'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,709] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,710] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,710] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,712] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'moody'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,714] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,717] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'had'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,718] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,720] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'worst'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,721] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'cramps'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,722] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,724] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'hair'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,725] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'loss'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,727] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,729] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'acne'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,730] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,730] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'horrible'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,731] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'headaches'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,731] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,732] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,732] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'appetite'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,732] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,733] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'a'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,733] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'sumo'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,733] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'wrestler'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,734] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,734] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '&'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,734] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'did'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 15:09:23,735] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'n't'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,735] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'want'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,735] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,736] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'do'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,736] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'anything'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,736] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,737] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,737] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'felt'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,738] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'lazy'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,738] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,739] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'borderline'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,739] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'depressed'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,740] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,741] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'They'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,742] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'do'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,744] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'n't'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,745] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'tell'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,746] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'you'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,747] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'those'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,748] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'side'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,748] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'effects'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,749] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,750] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'obvious'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,751] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'reasons'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,752] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,752] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Yesterday'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,753] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,753] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'finally'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,756] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'had'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,759] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'it'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,760] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'removed'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,762] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,763] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Being'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,764] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'traumatized'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 15:09:23,764] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'by'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,765] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'having'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,765] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'it'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,765] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'put'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,766] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,766] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,766] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'about'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,766] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'made'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,767] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'myself'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,767] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'sick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,767] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'with'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,768] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'nerves'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,768] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,768] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'The'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,769] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'removal'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,769] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,770] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'quick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,770] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,770] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'painless'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,771] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,772] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,772] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'already'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,773] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'feel'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,774] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'like'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,775] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'myself'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,779] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'again'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-05-26 15:09:23,780] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    }
   ],
   "source": [
    "t2 = text2tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bea4d324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6895bad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
